 

DIY guide to convert Speech-to-text with DeepSpeech AND Text-to-speech with WaveNet Kajal Puri
 
 
15:09:000668:000668_2 : 15:07:15:581: I'm Kajal Puri I am doing Masters of computer science at university of Grenoble alpes in France I have to join my masters after completing one year as a data scientist at fractal Analytics Bangalore India and today I am going to talk about speech recognition I'm very a little bit about it so these are my social media handles in case you want to reach out. 2
15:10:000669:000669_0 : 15:09:09:745: So basically I start from the speech recognition basic definition. So in day to day life of just like the Vision or language are most important part of human life is voice or speech or speech recognition is a when a computer can't detect whatever are you have said in whatever language you have said in whatever expressions you have said I delete it anyway should be able to . 3
15:10:000670:000670_4 : 15:09:42:863: wear when you have said accurately into the form of text so that speech recognition and it's applications are very general in like the form of automatic captions generated captions on YouTube or not on YouTube anymore I guess this is the perfect example of automatic captions so yeah II is the voicemail transcription it or used to exist probably like 5 to 6 years back and it was very . 4
15:11:000671:000671_1 : 15:10:16:003: These days, obviously, its accuracy has increased but because of all the internet revolution, the telephone companies telecom companies have stopped using that, are the tide application that is very widely used as Siri Cortana and Google home etc like or you can give instruction or to your Google Mini that you not switch on the lights and it'll probably interpret I . 1
15:11:000672:000672_0 : 15:10:45:135: If it's not even doing to bed it'll probably Google search for you and dictate the results of so these are some basic applications on. This particular diagram shows you the high level interpretation of how speech recognition systems work so what happens is generally whenever I hear a sentence of each word syllable or has a particular soundwave associated to it . 2
15:11:000673:000673_3 : 15:10:46:135: You did to it so whenever even when you're saying I'm hello sequence of sound waves will be generated which will then be detected by neural networks and of . 3
15:12:000674:000674_4 : 15:11:29:486: We are using many algorithms, like pattern recognition or deep learning algorithms you can identify or you can match that particular sound wave to the word or syllable order sentence and then you can predict what the . 4
15:12:000675:000675_0 : 15:11:47:633: Soundwave was. like in this case the soundwave was hello then when we pass it to the neural network after a lot of people singing background of matching I predict says "hello". So, generally the word here is very predictable so it is predicted accurately but when we are seeing a lot of words or sentences put together in a
15:12:000676:000676_2 : 15:12:16:787: just like this one . 2
15:13:000677:000677_3 : 15:12:18:903: So basically, ASR, basicaly this how ASR systems work of fast is the speed segment extraction so what happens is that when of when we are speaking we are not speaking a word or syllable when speaking in a continuous of chain of sentences of word and a housing deleted done is that you're a model or your job during the processing face of . 3
15:13:000678:000678_0 : 15:12:51:051: Each sentence will be segmented in different words, so let's say you have of you have acted a sentence that I am at Pyton conference so how computer will integrate is that it will segmented do I am at and bake on translate the segmentation can vary but yeah. Are little segment or your whole speech or the whole of what you have sentence. . 4
15:14:000679:000679_0 : 15:13:18:236: I think our techniques can be applied only to refine it. Let's say that when you are saying this in a public of public place and there will be a lot of background noise so you can you find it more or so that the background noise is lesser or you can enhance the quality of the sound to this these are there are a lot more steps that you can do in the signal or speech preprocessoring part . 1
15:14:000680:000680_0 : 15:13:46:352: And then there are two ways basically or to do the modelling so when is that you can already use the vector embeddings that are already available on the internet or the second is that you can train your own model as well. So if you are already using the of between vectors of a model then or what you can do if you can just take the input sample offer of audio files and you can apply. 2
15:15:000681:000681_0 : 15:14:16:477: Statistical matching techniques on it and probably will be able to get the answers so please answer in the centre prediction of whatever or you have a 3rd and the second is that you feel first get the training data and then train your own model are you can probably use any of Architecture. There on the internet are you can actually customise that. it takes a lot of time . 3
15:15:000682:000682_4 : 15:14:17:477: But in some cases it's totally worth it then after training and pattern matching you can get the output so yeah that make you can see it there . 4
15:15:000683:000683_1 : 15:14:56:256: So what are the challenges of ASR so the first challenge is as old is the human accent that we can see here as well if you would have noticed that in this system whenever I click click in the world we all have we all come from different worlds and human accent is a very of integral part of us so generally what happens is that the data set of . 1
15:16:000684:000684_2 : 15:15:28:390: Internet . 2
15:16:000685:000685_3 : 15:15:30:513: That is probably the American on British native speakers so that's why there is a lot of bias in the data said that is freely available already so that is why I love Siri will be able to detect or what an American has said much more accurately than what a French or an Indian person would be saying to it so that's a really big problem officer . 3
15:16:000686:000686_4 : 15:15:58:684: You can I just say something out a human cannot be a robot so that's why every human will have its own unique or boys or accent which is generally ignored by an system so that's a big problem that people are working on it . 4
15:16:000688:000688_2 : 15:16:15:811: Hm I want to show you a quick demo of what it looks like. 2
15:16:000689:000689_0 : 15:16:21:955: I will probably show it in the end, so that we don't have any technical difficulties so they're taking our problem off ASR is the background noise so are in an ideal situation he will be sitting in a room and saying something to Siri but that's not always going to be the case because you can be travelling or you can be in a
15:17:000690:000690_4 : 15:16:48:122: Most of . 4
15:17:000691:000691_1 : 15:16:51:259: Most of the data sets that are available on internet are very ideal, they do not have any background noise so you don't need much redefining but in a real-time it's a problem because whenever you are speaking you will have a little bit of most of the time they will have a little bit of background noise so sometimes it interferes and the system is not able to predicted correctly so . 1
15:18:000692:000692_2 : 15:17:16:397: This diagram shows the intersection of accent and of background noise and you can see that the most the most word error rate is when there is an Indian native speaker and it has a lot of backg finish which is pretty obvious . 2
15:18:000694:000694_3 : 15:17:38:374: Round noise Yeah so are the third is the misinterpretation of homophones so homophones are the words that sound exactly the same sound that might have different spellings as well but they have different meanings are the meaning of is different at different parts of so the perfect example if "We have won the match" and it can easily be predicted by any other system is we have won the match which door . 3
15:18:000695:000695_4 : 15:18:06:555: All of me totally changes the meaning of the sentence and there are a few number of more example so what happens if I probably will not Fade Like it's not the correct structure "we went to see the sea but in the human . 4
15:18:000696:000696_1 : 15:18:25:680: Can you also like you often see sentences that are not maybe English . 1
15:18:000697:000697_2 : 15:18:32:823: In the structure but that is used of very much in the conversation so these are the sum of points that will be used in the conversation and that you can actually tell Siri to do something but . 2
15:19:000698:000698_3 : 15:18:48:964: you can say that there may be grammatically or structurally not very English Direct so yeah . 3
15:19:000699:000699_4 : 15:18:56:116: Phoebe always or most of the time misinteprets homophones, so that is a problem that as I'd has 2 of a cup of tea I should not be having because in the league humans understand what we are trying to say . 4
15:20:000700:000700_1 : 15:19:13:259: So the fourth point is the overlapping sounds so what happens is that when humans are talking sometimes lots of different people are talking at the same time and still we are able to differentiate who is speaking but ASR is not able to answer one of the reason is the . 1
15:20:000701:000701_2 : 15:19:40:405: of the reason is the . 2
15:20:000702:000702_3 : 15:19:41:405: Unavailability of l the data sets are single speaker and single stream available and there is no overlapping and sometimes there are no multiple speakers so that's why it's strained or on such type of data sets and it gives0 results . 3
15:20:000704:000704_4 : 15:20:03:706: You should be able to a segment of of between the two personalities and who is trying to say what . 4
15:21:000705:000705_1 : 15:20:13:709: There are other minor problem in as ASR as weel the of waiting and my mentality that you are giving an instruction to an AI system and you're working from your home to outside so I probably in your home you will not have much noisy sounds but when you go outside you will probably have a noisy background and sometimes because of the genes in a little bit of in . 1
15:21:000706:000706_0 : 15:20:45:869: Yes I do feel it should not be the case ideally. The second is a very important part which is the artefact from the hard way to some be even these days your phones are not supposed to get the offer quality of whatever you were saying azan in the audio and because of the quality of the as I might not be able to detect the current sound waves and it might not understand what you're . 2
15:22:000707:000707_3 : 15:21:19:986: Are the top of the town is the audio and compression artefact this nearly happens not in the real time but let's say you're trying to transcript or . 3
15:22:000708:000708_4 : 15:21:35:131: Transcription system so I can be a good example of feedback when you have heard the whole ordeal and when you're trying to compress it then the quality of the audio can be lost there are many other factors as well like the age of the Speaker of a child have a different kind of . 4
15:22:000709:000709_1 : 15:21:36:131: Colander pronunciation of words or from Android 8 speaker . 1
15:22:000710:000710_2 : 15:22:03:477: What's the most important Factor days or high competition tools necessary so . 2
15:22:000711:000711_3 : 15:22:11:495: To convert a speech into a text requires a lot of . 3
15:23:000712:000712_4 : 15:22:17:631: Is a lot of processing preprocessing and save you will need high or am only be as well the data storage so that the issue in the ASR . 4
15:23:000713:000713_1 : 15:22:31:609: What is the need of a ASR so nowadays we know that there are going to be self-driving cars maybe after some time so the one thing is that of the can help you begin help you like in multitasking so let's say that when you are driving you can actually call system and your card that you know if you should turn on the light source which on Lake switch the temperature . 1
15:23:000714:000714_2 : 15:23:03:729: On air conditionning in you Car and much more stuff like that I'm taking this born for people with disabilities so take 10/:/ off right now human population is not is disabled in some way d and another for this is a boon for them that they don't have to use . 2
15:24:000715:000715_3 : 15:23:24:862: They don't have to use the lake fingers on hand in some way and only then . 3
15:24:000716:000716_4 : 15:23:32:996: They don't have to use the lake fingers on Understand them at all this can be used both my literate and illiterate which it is obvious I even a person who is not properly educating channel is communicate and the language so it will be very easy for them to give an instruction or ask a question to the automatic speech recognition system and they'll get the answer back or forward to very important point as a social perspective is that . 4
15:25:000717:000717_1 : 15:24:04:183: Languages in the world that are on the verge of extinction so let's say that you have a letter that Google has already released and yes a system which can understand a particular language and AJ language and then the people of those groups can actually or use it and they can help it to prevent or prevent the Extinction of that language and preserve it as it can be . 1
15:25:000718:000718_2 : 15:24:05:183: Taught to many people. 2
15:26:000719:000719_3 : 15:24:40:529:Speeding things instruction so this is also very important part because when you are trying to write to text or using some other way of communication it might be a little less faster than what the human speech can do so like you these days you don't even need a place you can just talk to your phone and probably your work will be done . 3
15:26:000720:000720_4 : 15:25:09:411: So are these are the three basic types of ASR the first is the template-based so it's it has been actually outdated so what happens in this case is you already have a set of template and if the person is trying to communicate with you then you can match the communication of that person with a particular template and if it does not match then you can sing . 4
15:26:000721:000721_0 : 15:25:10:511: To the person that the question answer that you're looking for is not in our system the second is the knowledge or rule-based this is applied in most of the IVR system that we called customer service and all they have a pre-order find a rules and if this if your question is out of the rules you cannot really communicate with them. so and you have a special a human that is employed the . 1
15:27:000722:000722_0 : 15:26:09:770: That is the second. And the third is statistical approach which is used most commonly these days and this is based on data Science particularly because you have to collect a lot of the time training it on a particular deep learning architecture then you apply a lot of pattern recognition or matching a statistical approaches and then you can get your result . 2
15:27:000723:000723_0 : 15:26:33:701: So like now how it can be done so there are two ways to do it one is to either program of model by yourself and the second is to use an existing API are the few apis that I have tasted so are the best API is the Google speech API but the problem is that it takes a lot of money to pay because I think after a up 2000 they are not free
15:28:000724:000724_4 : 15:27:06:830: so that can be a Big Challenge for students are for research purposes because you don't have that much of funds to invest there are fewer so basically the APIs also have different features and the IBM the IBM Watson API does not give you a very customised door with you as much as Google or Microsoft can I do for you and also in these apis you can Select Your Language season select different language . 4
15:28:000725:000725_1 : 15:27:37:995: Cancel a different of the frequency of different words that are spoken of these are the few features how deep speech is one of the most Heelys open source library which converts speech to text and it's based on the Baidu research paper and it is also implemented in Python and tensorflow . 1
15:29:000726:000726_2 : 15:28:00:009: So basically as a developer I have used deep speech and the reason is as follows First the development of these features totally open-source like you can quickly you can contribute are you can actually take the code of what are the features that are behind those functions ET secondaires Hypervenom metre tuning of modern so you can see them which type O . 2
15:29:000727:000727_3 : 15:28:30:156: Mini implemented and you can actually play with the hyperparameters according to the according to the task you need or the kind of dataset you have a tortoise cost effective because of its open source of nature are 4th is batch processing so a lot of APIs like Microsoft and Google they do not provide the service which is to batch process and check how much time or how much . 3
15:29:000728:000728_0 : 15:28:57:311: Alphabetical a badge of your data safe this is giving you can train your own model from scratch this is generally very expensive but in case if you have resources you can do it and I think this will be worth it if you have a really good data set or distributed training is also not done their pre-trained models so let's see that in case
15:31:000729:000729_1 : 15:29:24:501: Can you just want to get the already train embeddings and you just want to do use it on your data sets that you can do with it as well and it gives better results in general or with the real-time speech data which is really the problems we are working on to build and ideally I so this is the result comparison off all the apis . 1
15:31:000730:000730_2 : 15:29:51:627: The comparison is John and generally down on first between dataset and then the Noisy datasets so as to know if your model is not overfeed by a particular data set or it works equally well on a real-time human or data set . 2
15:31:000731:000731_4 : 15:30:57:924: You can a person doesn't know who is human and who is any of your system so it can easily distinguish who is an virtual assistant . 4
15:32:000732:000732_0 : 15:31:09:080: What they're trying to do is there trying to put more number of emotions and to make it sound more like humans that you're not able to distinguish if I like the who who was talking to you on the phone and this was probably the yeah this was demonstrated on the Google IO 2018 as well by Sundar Pichai so the major two parameters
15:32:000733:000733_2 : 15:31:36:260: No text to speech intelligibility and how humanely it sounds so intelligibility is generally a hardware or of the background problem it's like the quality of audio how clear it is and whether it's listenable or not which are not that tough to achieve them or at ufford Park is how Human League sounds so emotions the timing at the structure of sentences and as well the prononciation . 2
15:32:000734:000734_0 : 15:32:07:384: For these are the human features that are really from every other human 2 others so it's not just basically if you want an AI to sound like yourself you must they must know you how you speak and how you deliver your sentences are what kind of pronunciation you have. This is a problem that they are working on . 3
15:32:000735:000735_1 : 15:32:33:563: To solve this problem there are two architectures that are really popular these days when is the wavelength and second is the simple rnn so the wavelength of . 1
15:33:000736:000736_2 : 15:32:47:682: Picture is generally a . 2
15:33:000737:000737_0 : 15:32:51:831: They both models generator raw audio model this is an autoregressive generative models also what it
15:34:000738:000738_0 : 15:33:05:962: Is a speech is a sequence of audio signals and audio decoder recurrent neural networks are designed to do that. But it uses a convolutional neural networks instead and of today are not exactly the convolution neural networks by dilated in a way that they can order take the video sequences and of get the minimum sample of an audio and then they apply various pattern recognition algorithm . 4
15:34:000739:000739_1 : 15:33:40:034: So what the experiment washuman done on that it was trained on the human data and the output produced was just like human that it was able to do is able to detect the emotions the sounds that humans produce but it was not able to detect the right kind of text structure of saw it it was is blubbering and in one way or another . 1
15:34:000740:000740_0 : 15:34:11:162: The good and bad thing of this is that it takes 4 minutes to generate one second of audio .Some people that say that it's lesser for a start and some people say that it's really good for a start. To Fall Live ill have to look out for more research papers and I think down the line they will lakes are nearly listen to probably one minute or 30 seconds so let's see what is . 2
15:35:000741:000741_3 : 15:34:38:306: To be in future so this is the sort of evening which is not the usual convolution of the neural networks so this is the dilated convolution neural network which was just discovered to accomplish this task . 3
15:35:000742:000742_4 : 15:34:56:424: So this is the second type of architecture, this is Sample-RNN which is not explored much of to live up to the date of it is also used to generate the audio samples so what happens if that multiple are innings I finished it in a particular sequence and the audio is given to the top layer of the recurrent neural network and it gets Empire until it's at its . 4
15:36:000743:000743_1 : 15:35:25:569: Sample of to the last layer of the Architecture of so it is computationally 500 times faster but it does not give that much of good results as wavenet and . 1
15:36:000744:000744_0 : 15:35:41:688: But not that good stale so not much can be said about it because it has not been experimented. So if somebody a man who is looking to experiment with text to speech she can barely explored this idea more because . 2
15:36:000745:000745_3 : 15:35:58:833: Hazardous is architecture of simple rnn Deezer the slide actually that I just . 3
15:36:000746:000746_4 : 15:36:06:953: By attending the few tasks in the morning so this picture is the transcription generated by this module by an Indian speaker and you can do it so the deadlines are there soda read about sentences are probably the only totally not in the . 4
15:36:000747:000747_1 : 15:36:28:080: Early night in the . 1
15:36:000748:000748_2 : 15:36:31:215: Totally contrasting . 2
15:36:000749:000749_3 : 15:36:36:381: They don't ot at all with the things that the person was trying to or speak or interpret . 3
15:37:000750:000750_0 : 15:36:37:381: Totally match
15:37:000751:000751_4 : 15:36:47:510: If these are the things that was . 4
15:37:000752:000752_1 : 15:36:52:621: Spoken by a guy who has a very good English accent as compared to the another so it doesn't make sense of completely but I think it has still listen I'm off out of the box character of Ord so I think this is a very live demonstration of yourself . 1
15:38:000753:000753_2 : 15:37:14:686: For these are the references that I have included in this my diagrams and the results were taken from these resources and this is my email ID in case you have a question . 2
15:38:000754:000754_0 : 15:37:30:501: Snow so deep speech angry evening actor so basically this question is about how is wavenet is used in deep speech so deep speech is for now use just for the speech to text generation that is if you're speaking a text and accurate text should be generated. Whereas the Void wavenet is doing the opposite of converting a text to the speech . 3
15:38:000755:000755_4 : 15:38:17:638: Yeah so basically deep state deep speech has more votes are fighting as well as tensorflow implementation so . 4
15:38:000756:000756_1 : 15:38:32:776: Santa Fe NM . 1
15:38:000757:000757_2 : 15:38:37:924: I think I . 2
15:38:000758:000758_3 : 15:38:42:061: Kodi do this let me check . 3
15:39:000759:000759_4 : 15:38:47:209: Yeah so this is one article difference that I have taken . 4
15:39:000760:000760_1 : 15:38:55:901: Ok so I just added as a resource in her of the sample code that I have tried I just added her or you can just bring me on my email and I'll send it to your voice now . 1
15:40:000761:000761_2 : 15:39:11:461: Oh yes you think I'm good thanks for reminding me . 2
15:40:000762:000762_1 : 15:39:36:680: Ok . 1
15:42:000763:000763_3 : 15:39:55:586: Unmute it . 3
15:47:000764:000764_1 : 15:40:09:511: No . 1
15:51:000765:000765_3 : 15:41:35:543: The problem . 3
15:51:000766:000766_2 : 15:47:00:364: I know I haven't actually tried on any other model of deep sleep is the only one that was available as a free open-source and that is what we were looking at that time so there might be some place Google or Microsoft API for that but I haven't tried time so I cannot see me . 2
15:51:000767:000767_4 : 15:50:58:325: Tell me the . 4

 