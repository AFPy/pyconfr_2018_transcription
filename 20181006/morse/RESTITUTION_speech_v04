
Bringing Shakespeare back to life! Arnav Arora


13:45:000503:000503_0 : --------
13:45:000504:000504_0 : Rechargement du document--------


14:09:000512:000512_3 : 14:01:01:724: Waiting we have a chat fiction . 3
14:09:000513:000513_2 : 14:09:04:446: Next song to buy another but how to bring Shakespeare back to life . 2
14:09:000514:000514_3 : 14:09:30:582: Hey guys it's just learnt a lot of us are trying to buy you go to bed . 3
14:10:000515:000515_4 : 14:09:46:222: Ring sexy back to life just took its musical going to be a boxing in general and how we can use NLP . 4
14:10:000516:000516_1 : 14:09:56:350: Daily lives in our can improve our lives so I'm never . 1
14:10:000517:000517_2 : 14:10:02:484: India . 2
14:10:000518:000518_3 : 14:10:05:607: Skitarii . 3
14:11:000519:000519_4 : 14:10:09:061: Start off with this ring size how language is complicated maybe it's not very obvious to humans but when you look at video objective Leven you remove that morning they would be having a head take example this I ate pizza with olives and his friends now either way eating people now or lose an hour people so things get complicated when you take things out of context . 4
14:11:000520:000520_1 : 14:10:35:185: The thing I'm trying to see you an example would be local High School Dropout cut by half so now we know that we don't actually go around killing people in half so good sentences that number of people who dropped out at local schools droppet out by half an otherwise I so did you see a kid who has a telescope or through a telescope so . 1
14:11:000521:000521_2 : 14:11:02:315: When we are using language and giving up Giving In 24th your wages what you fancy doing a mind of building this model of the world and we take that for granted when we talk to each other and it's true that context so we can understand each other otherwise if objectives of communication . 2
14:12:000523:000523_3 : 14:11:28:594: Language So they just swan xkcd comic I really like her and it supports appoint what I'm trying to put across it makes highlighted so a person says to another . 3
14:12:000524:000524_0 : 14:11:39:732: Actually, I could care less and then I'll go and collect them I think you mean you couldn't care less send you could care less imply that you carefully Simone I don't know where is unbelievably complicated brains just intro oya trying in Vain to connect with one another and blindly flinging was Into
14:12:000525:000525_1 : 14:12:03:866: Contacting stop texting me and every listener interface science in their own way . 1
14:12:000526:000526_2 : 14:12:11:001: It's a glorious skill you never know if I work for sure what anybody any of the boys army 21 all you can do is try and get better at getting your words affect people so you can have a chance of finding one to make them feel something that the way you want them to feel basically everything is pointless . 2
14:12:000527:000527_3 : 14:12:29:134: She goes on to say "I assume you're giving me tips on how to interpret was because you want me to feel less alone if so thank you that means a lot to me" what you suggest running through my sentence is Rudimental checklist so you can show how well you know your grandma or language then it's basic I could care less . 3
14:13:000528:000528_1 : 14:12:45:308: So where does nlp comes language for . 1
14:13:000529:000529_2 : 14:12:52:424: That is some really talk to each other English French these are languages and the way you communicate with each other is called national language when we using languages to programme computer socket programming languages . 2
14:13:000530:000530_3 : 14:13:09:548: Of how we communicate with each other how to make it better how to understand it . 3
14:13:000531:000531_4 : 14:13:16:090: How do we do NLP? . 4
14:14:000532:000532_1 : 14:13:20:714: basically we can't just go on shouting whether the computer it's not going to make any sense to its what we have to do is the hardest different techniques called bag of words . 1
14:14:000533:000533_2 : 14:13:40:863: You can do is take something like a TF-IDF ( term frequency inverse document frequency ) we can you take a document and find out the number of times a word comes in that particular document or something like and watch which order become that's when the light comes and was himself so see b o w s country is Tiger Woods Instagram is it going to . 2
14:14:000534:000534_3 : 14:14:04:003: I'm going to keep them for now . 3
14:14:000535:000535_4 : 14:14:07:351: No sew bag of words manga what is basically a way of extracting information from the words from the sentences in the documents we already have . 4
14:14:000536:000536_1 : 14:14:18:486: Radio this one . 1
14:15:000537:000537_2 : 14:14:21:678: As you can see in the slide are there to bring your documents are we comparing the frequency of the words in those documents and we use other features from text then we can do other than machine and deep learning on them but the one disadvantage of back what does that information about all is lost and order in language means a lot to us because all I Was A linear so that's why the Sydney . 2
14:15:000538:000538_3 : 14:14:48:795: What's why this technique has been used int classical natural sourcing techniques used for I think the last 40 or 50 years but I was given the rising machine if this is what we don't use any more . 3
14:15:000540:000540_4 : 14:15:04:265: The Skip training for converting goes into these features that you can get into later . 4
14:15:000541:000541_1 : 14:15:12:414: How to get onto monster machine learning I think you're better . 1
14:15:000542:000542_2 : 14:15:18:564: I'm not going to go too much into it . 2
14:16:000543:000543_3 : 14:15:21:701: ML in the context of NLP we take those words in the document in the document we have and we can put them into features something that the models can understand and then we use supervisor answer why is ossification the main difference between machine learning and deep learning is that in . 3
14:16:000544:000544_4 : 14:15:46:834: Classical techniques like I'm in all day and another . 4
14:16:000545:000545_1 : 14:15:55:956: The thing with machines glass canopy Morris's be used to manually take out features from text things like tf-idf and word count and all the other ones I showed you but what happens in deep learning is it says manually giving it feature instead of telling the model that this is probably important in this isn't we let them do the work and that's beautiful a'll be no . 1
14:16:000546:000546_2 : 14:16:20:081: Calculate this is something that only make sense if This World Is Coming along then it's probably important to be let the machine do that working we just take that wasn't with them in a form the machine can understand . 2
14:17:000547:000547_3 : 14:16:34:008: How do we take these words and put them in a form machine can understand that would be embeddings because machines can't understand text because each character sketch me something to him but if you can go the character into ASCII number and any 7TJ there is something though machine can deal with . 3
14:17:000548:000548_4 : 14:16:53:754: Too many things so what we essentially doing this . 4
14:17:000549:000549_1 : 14:17:00:907: Model how is that we take these words and we represent them with arbitrary list of floating point numbers you can see I like this movie very much is represented by 5 different separate floating point numbers and we let the more I do the cleaning and learn what is the definition which is ideal for this kind of word so maybe of New York and Paris would be so similar in the Grave . 1
14:18:000550:000550_2 : 14:17:28:017: A nice picture from gloss paper I would you can do essentially after training these word embeddings to a moral I go to work or something what you can essentially do is take the word vector font and subtract the word vector from a woman from and ha and you get an ankle with Windows keyboard was it just blew everybody's mind . 2
14:18:000551:000551_3 : 14:17:53:140: That sort of understanding just follow these basic 5 decimal numbers is crazy just having that so . 3
14:18:000552:000552_4 : 14:18:04:295: What is essentially what rate in the context which we have inside a head which are embedded into this machine learning models and this is one way this is one with really good way of doing it . 4
14:18:000553:000553_1 : 14:18:17:063: When you come to NLP the most recently so that is going on different kind of problems because we can tackle just language in Genesis where you wash and re topic so we take specific small because . 1
14:18:000554:000554_2 : 14:18:31:199: Are in very small narrow context we can just sort everything you mean and mean what we try to be big things apart into small problem is also dictionary time and then he said we make boxes and through that . 2
14:19:000555:000555_3 : 14:18:47:332: Show and then we take before moonraker down to smaller problems . 3
14:19:000556:000556_4 : 14:18:54:468: Dealing with one which I'm going to be talking specifically about is language modelling which is essentially the task of predicting the next word what I'm doing right now essentially is automator language modeling I have to seeing everyone I'm thinking what to see the next tram station is pretty obvious from the name from one language to another question answering take  Take me to win reading comprehension texts and then be sent that text me need to answer questions how many cats were there what does it seem like something like that . 1
14:20:000558:000558_2 : 14:19:13:601: Text classification that would be something like what you do sentiment analysis so whether differences positive or negative weather Sidmouth sarcastic or not whether something seven or eight thoughts on this occasion . 2
14:20:000559:000559_3 : 14:19:40:988: How do I know if it's relation would be something like if I say John and William talking to each other he fell into a phone so who is he in this context is genre is the other person that's good . 3
14:20:000560:000560_4 : 14:19:41:988: Forest beating of the world what is an ounce in to understand all these things to learn a language better . 4
14:20:000561:000561_1 : 14:20:03:414: As I mentioned earlier language in our language English, French in the things we talk to each other with the sequential in nature so we come one after another and they only makes sense when they come one after another . 1
14:20:000562:000562_2 : 14:20:20:344: when we're using these modelswe trying to teach the smallest to understand a language what we do is we use sequential models because this is like a Time season 1 output when input after another and again do you know how much about what we can have a single output example of sentiment analysis sentence positive a block of text which is like 11 x app and then we go to the entire . 2
14:21:000563:000563_3 : 14:20:46:475: input match between Aston Villa translation where each word of couple of words together have a single word is the translation . 3
14:21:000564:000564_4 : 14:20:56:413: Now to see some examples of these sequences models article a _recurrent neural network . 4
14:22:000565:000565_1 : 14:21:05:571: Death of Eleanor Cox Joshua Feldman who sadly died this year in Eastleigh it be used as they call simplified Ireland and what they said she is so in what sequence is essentially are in Puerto different types test and what the greeting for letting your network cyst or what is the advantage of a hidden state where the thing how they have a hidden state across a if a sentence has something like . 1
14:22:000566:000566_2 : 14:21:36:729: "I am very happy" so that the one that came before "I" and the word this coming after am is it important for understanding what I'm representing the situation so what is initially we have here is a hidden say so whenever I'm giving an import I initially goes an egg 0 and goes in and out as X1 and very would be excuse and I'm just keeps going so we have a hidden state that . 2
14:22:000567:000567_3 : 14:22:02:872: Father Ted which is unticking all the sin and processing this information it's our last min we want to the next time step . 3
14:22:000568:000568_4 : 14:22:11:766: So when we're dealing with record new network save . 4
14:23:000569:000569_1 : 14:22:18:900: Sudden concrete very long sentences very large sequences what happened and let me just remind me to all these were through all these numbers calculating sending them back to . 1
14:23:000570:000570_2 : 14:22:34:049: Find out what is important what's not . 2
14:23:000571:000571_1 : 14:22:35:049:  The problem of vanishing gradient comes in Vito van Dyk we keep on multiplying something with point 1.1.1 it just explode or vanishes all that she is it when is 100 a dextrose and it's .1 or something in a decimal it explodes so very little things can we can be made to the recruiter network to the weight of the network is what we think she do text address that is in a way to New . 3
14:23:000572:000572_4 : 14:23:02:342: LFC how much is an ounce of memories which has a couple of a decent gates and Elaine Ford forget and awkward Gait which helps us in doing these things from remembering and remembering and forgetting the right kind things is implementation of the LSTM Gate and to give you an intuitive example see a picture and we are free this into the same what I said she would be cool and . 4
14:24:000573:000573_1 : 14:23:26:464: Understand what is it for getting the picture may be the location of the Pokémon Snorlax in this example isn't very important what I should say is that he looks like he looks in pain and what I should ignore so his dietary preferences are probably not very important in figuring out whether he's being attacked or not listening to the example if there's an input for getting a . 1
14:24:000574:000574_0 : 14:23:50:594: Ok moving onto unsupervised pretraining and I'm going to want to the landing of the dark but just to try on this. when we start a new task we don't start from scratch. when I'm when you talking save centre second section for example when I try to understand sarcasm when I wasn't really wanna tell you I sent had all the
14:24:000575:000575_0 : 14:24:16:751: So why should I monitor somebody so what which is it should I do here is he could ask in bed all the information of language are we have into that particular model and use of change model to perform a separate. And this is true to be very helpful and computer vision as well and now since they are playing. In language don't tell that's pretty useless . 3
14:25:000576:000576_4 : 14:24:40:879: Examples needed to attend rehab my patient . 4
14:25:000577:000577_1 : 14:24:47:413: So I can you search me for language modelling which is basically a task of predicting one word after another as you can see your initial come across one word and then the next one is we take the condition tropic of the four strong and second for the tour similar in the second word . 1
14:25:000578:000578_2 : 14:25:06:545: predict from a large vocabulary words which would be all the words in the document you have to figure out which word to say next which again let me find out I'm doing right now in my head but form order to do that it's very hard because we have lost Gilbert's disease play 3000 . 2
14:25:000580:000580_3 : 14:25:28:008: So what you might feel a bus or something like again on behalf another . 3
14:25:000581:000581_4 : 14:25:34:989: Is considered to be NLP imagenet. 4
14:26:000582:000582_1 : 14:25:38:118: That's the same field emission 80/:/ as someone I think one of the previous talk highlighted was one of the most influential moments and networks that use a normal ep 10 on a mission if that's what . 1
14:26:000583:000583_2 : 14:25:52:252: Now what can you do with language modelling since we have a large network and we change it on something like the entire works of Shakespeare in this example of allusion providing so on the surface of it is understood based on the millions of labelled as normal even if you're taking examples are in any task you'll need to provide machine learning or deep learning models examples . 2
14:26:000584:000584_3 : 14:26:17:386: This is what is supposed to be based on a number of features in language modelling the word itself says on the label so everyone at every time step you have the labour which is the next world so when you essentially have enormous large label did I said that you can't find any other task . 3
14:26:000585:000585_4 : 14:26:38:203: This language mode has been given inputs like this one which is from Shakespeare play . 4
14:27:000586:000586_1 : 14:26:44:564: Essentialy training when you start It just gives a gibberish something like this is a metre calculator can be with . 1
14:27:000587:000587_2 : 14:26:54:706: don't understand anything, there's no punctuation but as he moon what if you keep on training what happens is it start to understand again this dick so very very long time so in Morden and see what we try to do is use certain techniques like attention which he can see how well you have even small on your network within larger . 2
14:27:000588:000588_3 : 14:27:20:853: Figaro tension . 3
14:27:000589:000589_4 : 14:27:23:972: So this examples of tension I'm not going to go into details because that is another talk in itself . 4
14:28:000590:000590_1 : 14:27:30:119: How do something like a point of where to find do something like that so to help us understand and make the connacht optimisation easier and faster anyway chilli after a long time of training you get something like this which this is an output of a model before and I don't think I atleast couldn't shade this perform an actual Shakespeare text so that it says you can as you can see it in my  Zero Punctuation is . 2
14:28:000592:000592_3 : 14:27:56:253: Grammar is fine the words are similar to the way Shakespeare uses recovery . 3
14:29:000594:000594_4 : 14:28:05:618: So if you want to do some more this is language model which is been seen on recipes and it just makes up new recipes with something like half teaspoon of ground beer or a single point of sugar with ridiculous to us for language . 4
14:30:000597:000597_1 : 14:29:14:766: Yes of the question is inclusion task GPS are helpful but I am getting chased in straight losses is whether it is helpful in language or not and if it is and how so for computers domain networks with you as I mean I got the music . 1
14:32:000598:000598_2 : 14:29:53:900: Find a good thing about CNNs at the very least you can do parallel processing on GPUs and that doesn't work out here because what it is Libby using NLP it something like corrective exercise able to because we need to take things in order and an image the translation invariant so it doesn't matter which body image of paying attention to see all the other matters so . 2
14:32:000599:000599_0 : 14:30:18:068: It's hard but since this is been going on for 10 Years Eve come over the door techniques and models which can run fast so much much fat should I order of magnitude of 10 or 20 even using these models are Nintendo switch on GPS. . 3

 